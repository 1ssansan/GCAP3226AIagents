{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GCAP3226 Week 7: Web Crawler & API Workshop\n",
        "## Field Studies ‚Äì Project Data Collection\n",
        "\n",
        "**Date:** October 14, 2025 (Week 7)  \n",
        "**Duration:** 3 hours  \n",
        "**Focus:** Data Collection Techniques for Government Policy Analysis  \n",
        "**Course:** Empowering Citizens through Data: Participatory Policy Analysis for Hong Kong\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Workshop Objectives\n",
        "\n",
        "By the end of this workshop, students will be able to:\n",
        "1. **Understand APIs and Web Scraping** for government data collection\n",
        "2. **Implement basic web crawlers** using Python libraries\n",
        "3. **Access government APIs** for real-time data\n",
        "4. **Apply data collection techniques** to their specific team projects\n",
        "5. **Set up automated data collection** for ongoing project monitoring\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Workshop Agenda\n",
        "\n",
        "### Part 1: Introduction to Data Collection (45 minutes)\n",
        "- **Government Data Sources Overview** (15 min)\n",
        "- **APIs vs Web Scraping** (15 min)\n",
        "- **Legal and Ethical Considerations** (15 min)\n",
        "\n",
        "### Part 2: Hands-on API Workshop (60 minutes)\n",
        "- **Hong Kong Government APIs** (20 min)\n",
        "- **Weather Data APIs** (20 min)\n",
        "- **Transport Data APIs** (20 min)\n",
        "\n",
        "### Part 3: Web Scraping Fundamentals (60 minutes)\n",
        "- **BeautifulSoup Introduction** (20 min)\n",
        "- **Requests Library** (20 min)\n",
        "- **Practical Scraping Exercise** (20 min)\n",
        "\n",
        "### Part 4: Team-Specific Application (45 minutes)\n",
        "- **Team Breakout Sessions** (30 min)\n",
        "- **Data Collection Planning** (15 min)\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Technical Setup\n",
        "\n",
        "### Required Software\n",
        "```bash\n",
        "# Install required Python packages\n",
        "pip install requests beautifulsoup4 pandas matplotlib seaborn\n",
        "pip install hko-weather-api  # Hong Kong Observatory API\n",
        "```\n",
        "\n",
        "### Development Environment\n",
        "- **Jupyter Notebook** or **Google Colab**\n",
        "- **Python 3.8+**\n",
        "- **Internet connection** for API access\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Government Data Sources\n",
        "\n",
        "### Hong Kong Government APIs\n",
        "\n",
        "#### 1. Hong Kong Observatory (HKO) API\n",
        "- **Weather Data**: Real-time weather conditions\n",
        "- **Typhoon Information**: Tropical cyclone tracking\n",
        "- **Air Quality**: AQHI data and forecasts\n",
        "\n",
        "#### 2. Transport Department APIs\n",
        "- **KMB Bus Routes**: Route information and schedules\n",
        "- **Bus Stops**: Stop locations and facilities\n",
        "- **Real-time Arrival**: Live bus arrival times\n",
        "\n",
        "#### 3. Data.gov.hk\n",
        "- **Open Data Portal**: Comprehensive government datasets\n",
        "- **Department-specific data**: Various government departments\n",
        "- **Historical data**: Long-term trend analysis\n",
        "\n",
        "### Team-Specific Data Sources\n",
        "\n",
        "#### Team 1: Flu Shot Campaign Analysis\n",
        "- **Department of Health**: Vaccination statistics\n",
        "- **Hospital Authority**: Healthcare utilization data\n",
        "- **Census and Statistics**: Population demographics\n",
        "\n",
        "#### Team 2: Bus Route Coordination\n",
        "- **Transport Department**: Route performance data\n",
        "- **KMB/CTB**: Real-time bus data\n",
        "- **HKO**: Weather impact on ridership\n",
        "\n",
        "#### Team 3: Typhoon Preparedness\n",
        "- **Hong Kong Observatory**: Weather and typhoon data\n",
        "- **Security Bureau**: Emergency response protocols\n",
        "- **Census and Statistics**: Population distribution\n",
        "\n",
        "#### Team 4: Municipal Solid Waste Charging\n",
        "- **Environmental Protection Department**: Waste statistics\n",
        "- **Food and Environmental Hygiene**: Collection data\n",
        "- **Census and Statistics**: Household characteristics\n",
        "\n",
        "#### Team 5: Green @ Community Initiatives\n",
        "- **Environmental Protection Department**: Environmental data\n",
        "- **Home Affairs Department**: Community programs\n",
        "- **Development Bureau**: Urban planning data\n",
        "\n",
        "#### Team 6: Bus Stop Optimization\n",
        "- **Transport Department**: Bus stop utilization\n",
        "- **Planning Department**: Urban development plans\n",
        "- **Census and Statistics**: Population density\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 1: Setup and Import Libraries\n",
        "# Install required packages (run this cell first)\n",
        "# !pip install requests beautifulsoup4 pandas matplotlib seaborn\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(\"üìä Ready to start data collection workshop\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Hong Kong Government API Examples\n",
        "\n",
        "### 2.1 Hong Kong Observatory (HKO) Weather API\n",
        "\n",
        "Let's start with the Hong Kong Observatory API to get real-time weather data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HKO Weather API Class\n",
        "class HKOWeatherAPI:\n",
        "    \"\"\"Hong Kong Observatory Weather API Interface\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.base_url = \"https://data.weather.gov.hk/weatherAPI/opendata/weather.php\"\n",
        "    \n",
        "    def get_current_weather(self):\n",
        "        \"\"\"Get current weather data\"\"\"\n",
        "        params = {\n",
        "            'dataType': 'rhrread',  # Real-time weather data\n",
        "            'lang': 'en'\n",
        "        }\n",
        "        return self._make_request(params)\n",
        "    \n",
        "    def get_typhoon_info(self):\n",
        "        \"\"\"Get typhoon information\"\"\"\n",
        "        params = {\n",
        "            'dataType': 'tcm',  # Tropical cyclone information\n",
        "            'lang': 'en'\n",
        "        }\n",
        "        return self._make_request(params)\n",
        "    \n",
        "    def get_air_quality(self):\n",
        "        \"\"\"Get air quality data\"\"\"\n",
        "        params = {\n",
        "            'dataType': 'aqhi',  # Air quality health index\n",
        "            'lang': 'en'\n",
        "        }\n",
        "        return self._make_request(params)\n",
        "    \n",
        "    def _make_request(self, params):\n",
        "        \"\"\"Make API request with error handling\"\"\"\n",
        "        try:\n",
        "            response = requests.get(self.base_url, params=params, timeout=30)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            else:\n",
        "                print(f\"API Error: {response.status_code}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Request failed: {e}\")\n",
        "            return None\n",
        "\n",
        "# Test the HKO API\n",
        "print(\"üå§Ô∏è Testing Hong Kong Observatory API...\")\n",
        "weather_api = HKOWeatherAPI()\n",
        "\n",
        "# Get current weather\n",
        "weather_data = weather_api.get_current_weather()\n",
        "if weather_data:\n",
        "    print(\"‚úÖ Weather API working!\")\n",
        "    print(f\"üìä Available data keys: {list(weather_data.keys())}\")\n",
        "else:\n",
        "    print(\"‚ùå Weather API failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display weather data in a readable format\n",
        "if weather_data:\n",
        "    print(\"üå§Ô∏è Current Weather Data:\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Display temperature data\n",
        "    if 'temperature' in weather_data:\n",
        "        temp_data = weather_data['temperature']\n",
        "        print(f\"üå°Ô∏è Temperature: {temp_data.get('data', [{}])[0].get('value', 'N/A')}¬∞C\")\n",
        "    \n",
        "    # Display humidity data\n",
        "    if 'humidity' in weather_data:\n",
        "        humidity_data = weather_data['humidity']\n",
        "        print(f\"üíß Humidity: {humidity_data.get('data', [{}])[0].get('value', 'N/A')}%\")\n",
        "    \n",
        "    # Display wind data\n",
        "    if 'wind' in weather_data:\n",
        "        wind_data = weather_data['wind']\n",
        "        print(f\"üí® Wind: {wind_data.get('data', [{}])[0].get('value', 'N/A')}\")\n",
        "    \n",
        "    print(\"=\" * 40)\n",
        "    print(\"üìä Full data structure:\")\n",
        "    print(json.dumps(weather_data, indent=2)[:500] + \"...\")\n",
        "else:\n",
        "    print(\"‚ùå No weather data available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Transport Department API\n",
        "\n",
        "Now let's explore the Transport Department APIs for bus route and stop information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transport Department API Class\n",
        "class TransportAPI:\n",
        "    \"\"\"Transport Department API Interface\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.kmb_base = \"https://data.etabus.gov.hk/v1/transport/kmb\"\n",
        "        self.ctb_base = \"https://data.etabus.gov.hk/v1/transport/ctb\"\n",
        "    \n",
        "    def get_bus_routes(self, company='kmb'):\n",
        "        \"\"\"Get all bus routes\"\"\"\n",
        "        url = f\"{self.kmb_base}/route\" if company == 'kmb' else f\"{self.ctb_base}/route\"\n",
        "        return self._make_request(url)\n",
        "    \n",
        "    def get_bus_stops(self, company='kmb'):\n",
        "        \"\"\"Get all bus stops\"\"\"\n",
        "        url = f\"{self.kmb_base}/stop\" if company == 'kmb' else f\"{self.ctb_base}/stop\"\n",
        "        return self._make_request(url)\n",
        "    \n",
        "    def get_bus_arrival(self, stop_id, route, company='kmb'):\n",
        "        \"\"\"Get real-time bus arrival information\"\"\"\n",
        "        url = f\"{self.kmb_base}/eta/{stop_id}/{route}\" if company == 'kmb' else f\"{self.ctb_base}/eta/{stop_id}/{route}\"\n",
        "        return self._make_request(url)\n",
        "    \n",
        "    def _make_request(self, url):\n",
        "        \"\"\"Make API request with error handling\"\"\"\n",
        "        try:\n",
        "            response = requests.get(url, timeout=30)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            else:\n",
        "                print(f\"API Error: {response.status_code}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Request failed: {e}\")\n",
        "            return None\n",
        "\n",
        "# Test the Transport API\n",
        "print(\"üöå Testing Transport Department API...\")\n",
        "transport_api = TransportAPI()\n",
        "\n",
        "# Get bus routes\n",
        "routes = transport_api.get_bus_routes()\n",
        "if routes:\n",
        "    print(\"‚úÖ Transport API working!\")\n",
        "    print(f\"üìä Available routes: {len(routes.get('data', []))}\")\n",
        "    \n",
        "    # Show first few routes\n",
        "    if 'data' in routes and len(routes['data']) > 0:\n",
        "        print(\"üöå Sample routes:\")\n",
        "        for i, route in enumerate(routes['data'][:5]):\n",
        "            print(f\"  {i+1}. Route {route.get('route', 'N/A')}: {route.get('dest_en', 'N/A')}\")\n",
        "else:\n",
        "    print(\"‚ùå Transport API failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Web Scraping Fundamentals\n",
        "\n",
        "### 3.1 Introduction to BeautifulSoup\n",
        "\n",
        "Web scraping allows us to extract data from websites when APIs are not available. Let's learn the basics with BeautifulSoup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Web Scraping Example: Government News\n",
        "class GovernmentWebScraper:\n",
        "    \"\"\"Web scraper for government websites\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "    \n",
        "    def scrape_news_announcements(self, url, max_pages=5):\n",
        "        \"\"\"Scrape government news and announcements\"\"\"\n",
        "        all_news = []\n",
        "        \n",
        "        for page in range(1, max_pages + 1):\n",
        "            try:\n",
        "                page_url = f\"{url}?page={page}\"\n",
        "                response = requests.get(page_url, headers=self.headers, timeout=30)\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                \n",
        "                # Extract news items (adjust selectors based on actual website)\n",
        "                news_items = soup.find_all('div', class_='news-item')\n",
        "                \n",
        "                for item in news_items:\n",
        "                    title = item.find('h3').text.strip() if item.find('h3') else 'No title'\n",
        "                    date = item.find('span', class_='date').text.strip() if item.find('span', class_='date') else 'No date'\n",
        "                    link = item.find('a')['href'] if item.find('a') else 'No link'\n",
        "                    \n",
        "                    all_news.append({\n",
        "                        'title': title,\n",
        "                        'date': date,\n",
        "                        'link': link,\n",
        "                        'scraped_at': datetime.now().isoformat()\n",
        "                    })\n",
        "                \n",
        "                # Be respectful - add delay between requests\n",
        "                time.sleep(random.uniform(1, 3))\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Error scraping page {page}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        return pd.DataFrame(all_news)\n",
        "\n",
        "# Example: Scrape a simple government page\n",
        "print(\"üï∑Ô∏è Web Scraping Example\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Create a simple HTML example for demonstration\n",
        "sample_html = \"\"\"\n",
        "<html>\n",
        "<body>\n",
        "    <div class=\"news-item\">\n",
        "        <h3>Government Announces New Policy</h3>\n",
        "        <span class=\"date\">2025-10-14</span>\n",
        "        <a href=\"/news/1\">Read more</a>\n",
        "    </div>\n",
        "    <div class=\"news-item\">\n",
        "        <h3>Public Consultation Opens</h3>\n",
        "        <span class=\"date\">2025-10-13</span>\n",
        "        <a href=\"/news/2\">Read more</a>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Parse the HTML\n",
        "soup = BeautifulSoup(sample_html, 'html.parser')\n",
        "news_items = soup.find_all('div', class_='news-item')\n",
        "\n",
        "print(\"üì∞ Extracted News Items:\")\n",
        "for i, item in enumerate(news_items, 1):\n",
        "    title = item.find('h3').text.strip()\n",
        "    date = item.find('span', class_='date').text.strip()\n",
        "    link = item.find('a')['href']\n",
        "    print(f\"{i}. {title} ({date}) - {link}\")\n",
        "\n",
        "print(\"‚úÖ Web scraping demonstration complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Team-Specific Data Collection\n",
        "\n",
        "### 4.1 Team Data Collection Functions\n",
        "\n",
        "Now let's create team-specific data collection functions for each project.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Team-Specific Data Collection Functions\n",
        "class TeamDataCollectors:\n",
        "    \"\"\"Team-specific data collection functions\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def team1_flu_shot_data():\n",
        "        \"\"\"Team 1: Flu shot campaign data collection\"\"\"\n",
        "        print(\"üíâ Team 1: Flu Shot Campaign Analysis\")\n",
        "        print(\"üìä Data sources: Department of Health, Hospital Authority\")\n",
        "        print(\"üîç Focus: Vaccination rates, demographic patterns, policy effectiveness\")\n",
        "        # Implementation would go here\n",
        "        return {\"status\": \"ready\", \"data_sources\": [\"DOH\", \"HA\", \"Census\"]}\n",
        "    \n",
        "    @staticmethod\n",
        "    def team2_bus_route_data():\n",
        "        \"\"\"Team 2: Bus route coordination data collection\"\"\"\n",
        "        print(\"üöå Team 2: Bus Route Coordination\")\n",
        "        print(\"üìä Data sources: Transport Department, KMB/CTB APIs, Weather data\")\n",
        "        \n",
        "        transport_api = TransportAPI()\n",
        "        weather_api = HKOWeatherAPI()\n",
        "        \n",
        "        # Get bus routes\n",
        "        routes = transport_api.get_bus_routes()\n",
        "        \n",
        "        # Get weather data for correlation\n",
        "        weather = weather_api.get_current_weather()\n",
        "        \n",
        "        return {\n",
        "            'routes': routes,\n",
        "            'weather': weather,\n",
        "            'status': 'collected'\n",
        "        }\n",
        "    \n",
        "    @staticmethod\n",
        "    def team3_typhoon_data():\n",
        "        \"\"\"Team 3: Typhoon preparedness data collection\"\"\"\n",
        "        print(\"üåÄ Team 3: Typhoon Preparedness & Emergency Management\")\n",
        "        print(\"üìä Data sources: Hong Kong Observatory, Security Bureau\")\n",
        "        \n",
        "        weather_api = HKOWeatherAPI()\n",
        "        \n",
        "        # Get typhoon information\n",
        "        typhoon_info = weather_api.get_typhoon_info()\n",
        "        \n",
        "        # Get current weather\n",
        "        current_weather = weather_api.get_current_weather()\n",
        "        \n",
        "        return {\n",
        "            'typhoon': typhoon_info,\n",
        "            'weather': current_weather,\n",
        "            'status': 'collected'\n",
        "        }\n",
        "    \n",
        "    @staticmethod\n",
        "    def team4_waste_data():\n",
        "        \"\"\"Team 4: Municipal solid waste charging data collection\"\"\"\n",
        "        print(\"üóëÔ∏è Team 4: Municipal Solid Waste Charging\")\n",
        "        print(\"üìä Data sources: Environmental Protection Department, Census data\")\n",
        "        print(\"üîç Focus: Waste generation patterns, charging effectiveness\")\n",
        "        return {\"status\": \"ready\", \"data_sources\": [\"EPD\", \"Census\", \"FEHD\"]}\n",
        "    \n",
        "    @staticmethod\n",
        "    def team5_green_community_data():\n",
        "        \"\"\"Team 5: Green @ Community data collection\"\"\"\n",
        "        print(\"üå± Team 5: Green @ Community Initiatives\")\n",
        "        print(\"üìä Data sources: Environmental Protection Department, Home Affairs\")\n",
        "        print(\"üîç Focus: Community engagement, environmental impact\")\n",
        "        return {\"status\": \"ready\", \"data_sources\": [\"EPD\", \"HAD\", \"DevB\"]}\n",
        "    \n",
        "    @staticmethod\n",
        "    def team6_bus_stop_data():\n",
        "        \"\"\"Team 6: Bus stop optimization data collection\"\"\"\n",
        "        print(\"üöè Team 6: Bus Stop Optimization\")\n",
        "        print(\"üìä Data sources: Transport Department, Planning Department\")\n",
        "        \n",
        "        transport_api = TransportAPI()\n",
        "        \n",
        "        # Get bus stops\n",
        "        stops = transport_api.get_bus_stops()\n",
        "        \n",
        "        # Get bus routes\n",
        "        routes = transport_api.get_bus_routes()\n",
        "        \n",
        "        return {\n",
        "            'stops': stops,\n",
        "            'routes': routes,\n",
        "            'status': 'collected'\n",
        "        }\n",
        "\n",
        "# Test team data collection\n",
        "print(\"üéØ Testing Team Data Collection Functions\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test each team's data collection\n",
        "teams = [\n",
        "    TeamDataCollectors.team1_flu_shot_data,\n",
        "    TeamDataCollectors.team2_bus_route_data,\n",
        "    TeamDataCollectors.team3_typhoon_data,\n",
        "    TeamDataCollectors.team4_waste_data,\n",
        "    TeamDataCollectors.team5_green_community_data,\n",
        "    TeamDataCollectors.team6_bus_stop_data\n",
        "]\n",
        "\n",
        "for i, team_func in enumerate(teams, 1):\n",
        "    print(f\"\\n--- Team {i} ---\")\n",
        "    result = team_func()\n",
        "    print(f\"Status: {result.get('status', 'unknown')}\")\n",
        "    if 'data_sources' in result:\n",
        "        print(f\"Data sources: {', '.join(result['data_sources'])}\")\n",
        "\n",
        "print(\"\\n‚úÖ All team data collection functions tested!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Data Processing and Visualization\n",
        "\n",
        "### 5.1 Data Cleaning and Processing\n",
        "\n",
        "Let's create utilities for processing the collected data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Processing and Visualization Utilities\n",
        "class DataProcessor:\n",
        "    \"\"\"Data processing and cleaning utilities\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def clean_government_data(df):\n",
        "        \"\"\"Clean government data with common issues\"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return df\n",
        "            \n",
        "        # Remove duplicates\n",
        "        df = df.drop_duplicates()\n",
        "        \n",
        "        # Handle missing values\n",
        "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
        "        \n",
        "        # Standardize date formats\n",
        "        date_columns = df.select_dtypes(include=['object']).columns\n",
        "        for col in date_columns:\n",
        "            if 'date' in col.lower() or 'time' in col.lower():\n",
        "                try:\n",
        "                    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "                except:\n",
        "                    pass\n",
        "        \n",
        "        # Remove extreme outliers (99th percentile)\n",
        "        numeric_columns = df.select_dtypes(include=['number']).columns\n",
        "        for col in numeric_columns:\n",
        "            if len(df[col].dropna()) > 0:\n",
        "                q99 = df[col].quantile(0.99)\n",
        "                df = df[df[col] <= q99]\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_time_series(df, date_col, value_col, freq='D'):\n",
        "        \"\"\"Create time series data\"\"\"\n",
        "        if df is None or df.empty:\n",
        "            return None\n",
        "            \n",
        "        df[date_col] = pd.to_datetime(df[date_col])\n",
        "        df = df.set_index(date_col)\n",
        "        ts = df[value_col].resample(freq).mean()\n",
        "        return ts\n",
        "\n",
        "class DataVisualizer:\n",
        "    \"\"\"Data visualization utilities\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def plot_time_series(data, title, xlabel, ylabel):\n",
        "        \"\"\"Plot time series data\"\"\"\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(data.index, data.values)\n",
        "        plt.title(title)\n",
        "        plt.xlabel(xlabel)\n",
        "        plt.ylabel(ylabel)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    @staticmethod\n",
        "    def plot_correlation_matrix(df, title):\n",
        "        \"\"\"Plot correlation matrix\"\"\"\n",
        "        if df is None or df.empty:\n",
        "            print(\"No data to plot\")\n",
        "            return\n",
        "            \n",
        "        plt.figure(figsize=(10, 8))\n",
        "        correlation_matrix = df.corr()\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "        plt.title(title)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    @staticmethod\n",
        "    def plot_distribution(data, title, bins=30):\n",
        "        \"\"\"Plot data distribution\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(data, bins=bins, alpha=0.7, edgecolor='black')\n",
        "        plt.title(title)\n",
        "        plt.xlabel('Value')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Example: Create sample data and demonstrate processing\n",
        "print(\"üìä Data Processing and Visualization Demo\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create sample data\n",
        "sample_data = {\n",
        "    'date': pd.date_range('2025-01-01', periods=100, freq='D'),\n",
        "    'temperature': np.random.normal(25, 5, 100),\n",
        "    'humidity': np.random.normal(70, 10, 100),\n",
        "    'pressure': np.random.normal(1013, 20, 100)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(sample_data)\n",
        "print(f\"üìà Sample dataset created: {df.shape}\")\n",
        "print(f\"üìÖ Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "\n",
        "# Clean the data\n",
        "df_clean = DataProcessor.clean_government_data(df)\n",
        "print(f\"üßπ Data cleaned: {df_clean.shape}\")\n",
        "\n",
        "# Create time series\n",
        "ts = DataProcessor.create_time_series(df_clean, 'date', 'temperature')\n",
        "print(f\"üìà Time series created: {len(ts)} data points\")\n",
        "\n",
        "print(\"‚úÖ Data processing utilities ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Workshop Summary and Next Steps\n",
        "\n",
        "### 6.1 What We've Learned\n",
        "\n",
        "In this workshop, we've covered:\n",
        "\n",
        "1. **Government APIs**: Accessing Hong Kong Observatory and Transport Department data\n",
        "2. **Web Scraping**: Using BeautifulSoup to extract data from websites\n",
        "3. **Data Processing**: Cleaning and organizing collected data\n",
        "4. **Team-Specific Applications**: Customized data collection for each project\n",
        "\n",
        "### 6.2 Next Steps for Your Team Project\n",
        "\n",
        "1. **Choose Your Data Sources**: Select the most relevant APIs and websites for your project\n",
        "2. **Implement Data Collection**: Use the templates provided to collect your data\n",
        "3. **Clean and Process**: Apply the data cleaning utilities to prepare your data\n",
        "4. **Analyze and Visualize**: Use the visualization tools to explore your data\n",
        "5. **Document Your Process**: Keep track of your data collection methods and sources\n",
        "\n",
        "### 6.3 Resources and Support\n",
        "\n",
        "- **Code Templates**: Available in `Week7_Code_Templates.py`\n",
        "- **Quick Reference**: See `Week7_Quick_Reference.md`\n",
        "- **Workshop Summary**: Review `Week7_Workshop_Summary.md`\n",
        "- **Team Materials**: Check your team folder for project-specific resources\n",
        "\n",
        "### 6.4 Legal and Ethical Considerations\n",
        "\n",
        "Remember to:\n",
        "- **Respect rate limits** when accessing APIs\n",
        "- **Follow robots.txt** guidelines for web scraping\n",
        "- **Cite your data sources** in your final report\n",
        "- **Be mindful of data privacy** and usage terms\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Workshop Complete!\n",
        "\n",
        "You now have the tools and knowledge to collect data for your GCAP3226 team project. Use the templates and examples provided to build your data collection pipeline.\n",
        "\n",
        "**Good luck with your projects!** üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
